# -*- coding: utf-8 -*-
"""machine learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KsaKwXnK-y7dvpdfH2lTwkppQHu1IHsT
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

ds=pd.read_csv('bank-direct-marketing-campaigns.csv')

ds

# Set the style for Seaborn plots
sns.set(style="whitegrid")

# Visualize the distribution of the target variable 'y'
plt.figure(figsize=(8, 5))
sns.countplot(x='y', data=ds, palette='viridis')
plt.title('Distribution of the Target Variable (y)')
plt.xlabel('Subscription (Yes/No)')
plt.ylabel('Count')
plt.show()

# Visualize the distribution of numerical features (e.g., age)
plt.figure(figsize=(12, 6))
sns.histplot(ds['age'], bins=30, kde=True, color='skyblue')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Count')
plt.show()

# Visualize the distribution of education levels
plt.figure(figsize=(10, 6))
sns.countplot(x='education', data=ds, palette='pastel', order=ds['education'].value_counts().index)
plt.title('Distribution of Education Levels')
plt.xlabel('Education Level')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# Boxplot for numerical features grouped by 'y' (target variable)
plt.figure(figsize=(14, 8))
sns.boxplot(x='y', y='age', data=ds, palette='Set3')
plt.title('Boxplot of Age Grouped by Subscription')
plt.xlabel('Subscription (Yes/No)')
plt.ylabel('Age')
plt.show()

# Countplot for job categories
plt.figure(figsize=(14, 8))
sns.countplot(x='job', data=ds, palette='viridis', order=ds['job'].value_counts().index)
plt.title('Count of Jobs')
plt.xlabel('Job')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

print(ds.head())

ds['education'].value_counts()

"""# Data Processing

# Check For Missing Values
"""

# Check for missing values
missing_values = ds.isnull().sum()
print("\nMissing Values:")
print(missing_values)

ds.info()

ds.isnull().any

# Check for and remove duplicates
print("Number of duplicates before removing:", ds.duplicated().sum())
ds = ds.drop_duplicates()
print("Number of duplicates after removing:", ds.duplicated().sum())

# Check for missing data (non-null but potentially incorrect or placeholder values)
missing_data = ds.isin(['unknown', 'other', '999']).sum()
print("\nMissing Data (e.g., 'unknown', 'other', '999'):")
print(missing_data)

# Check the distribution of the target classes
print(ds['y'].value_counts())

"""# **# **Encode Categorical Values****"""

# Convert categorical features to numerical using Label Encoding
label_encoder = LabelEncoder()
categorical_columns = ds.select_dtypes(include=['object']).columns

for col in categorical_columns:
    ds[col] = label_encoder.fit_transform(ds[col])

# Display the first few rows after encoding
print("\nEncoded Dataset:")
print(ds.head())

ds

ds=ds.drop(['pdays'], axis=1)

"""# **Balance the in inbalanced data**"""

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
import pandas as pd

# Assuming 'ds' is your original dataset
# X should contain the features, and 'y' should be the target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, ds['y'], test_size=0.2, random_state=42)

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the training data only
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Concatenate the synthetic samples with the original training set
X_train_balanced = pd.concat([X_train, X_train_smote], axis=0)
y_train_balanced = pd.concat([y_train, y_train_smote], axis=0)

# Now, X_train_balanced and y_train_balanced contain the balanced training set

# Assuming 'y_train_balanced' is the target variable after oversampling

# Display the class distribution after oversampling
print(y_train_balanced.value_counts())

# Create a bar plot for the class distribution
plt.figure(figsize=(8, 5))
sns.countplot(x=y_train_balanced, palette='viridis')
plt.title('Class Distribution After Oversampling')
plt.xlabel('Subscription (Yes/No)')
plt.ylabel('Count')
plt.show()

# Assuming 'ds' is your DataFrame
# Check for duplicates in the entire DataFrame
duplicates = ds.duplicated()

# Print the count of duplicate rows
print("Number of duplicate rows: {}".format(duplicates.sum()))

# If you want to display the duplicate rows, you can use:
# duplicate_rows = ds[duplicates]
# print(duplicate_rows)

# Assuming 'ds' is your DataFrame
# Remove duplicates based on specific columns (excluding 'y' column)
ds_no_duplicates = ds.drop_duplicates(subset=ds.columns.difference(['y']))

# Print the shape of the DataFrame before and after removing duplicates
print("Shape before removing duplicates: {}".format(ds.shape))
print("Shape after removing duplicates: {}".format(ds_no_duplicates.shape))

# Impute missing numeric values with the mean
numerical_imputer = SimpleImputer(strategy='mean')
ds['age'] = numerical_imputer.fit_transform(ds[['age']])

# Impute missing categorical values with the most frequent category for other columns
categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan']
for col in categorical_columns:
    categorical_imputer = SimpleImputer(strategy='most_frequent')
    ds[col] = categorical_imputer.fit_transform(ds[[col]])

# Treat missing binary values as a separate category
binary_columns = ['default', 'housing', 'loan']
for col in binary_columns:
    ds[col] = ds[col].fillna('unknown')

# Replace '999' with NaN in relevant columns
categorical_999_columns = [ 'previous', 'poutcome']
ds[categorical_999_columns] = ds[categorical_999_columns].replace('999', pd.NA)

# Replace 'unknown' and 'other' with NaN
ds.replace(['unknown', 'other'], pd.NA, inplace=True)

# Impute missing categorical values with the most frequent category
categorical_imputer = SimpleImputer(strategy='most_frequent')
ds[['job', 'marital', 'education']] = categorical_imputer.fit_transform(ds[['job', 'marital', 'education']])

# Verify that missing values are handled
missing_values_after_imputation = ds.isnull().sum()
print("\nMissing Values After Imputation:")
print(missing_values_after_imputation)

"""# **Splitting the dataset**"""

# Define features (X) and target variable (y)
X = ds.drop('y', axis=1)  # Features
y = ds['y']  # Target variable

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=20)

# Display the shapes of the training and testing sets
  print("\nShapes of Training and Testing Sets:")
  print("X_train:", X_train.shape)
  print("X_test:", X_test.shape)
  print("y_train:", y_train.shape)
  print("y_test:", y_test.shape)

"""# **Initialize the Random Forest Classifier**"""

# Initialize the Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=20)

# Train the classifier
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", classification_rep)

"""# **Implement the GridSearch For RandomForest**



"""

from sklearn.model_selection import GridSearchCV

# Example grid search for RandomForestClassifier
param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

grid_search = GridSearchCV(RandomForestClassifier(random_state=20), param_grid, cv=5)
grid_search.fit(X_train, y_train)

# Best hyperparameters
best_params = grid_search.best_params_
print("Best Hype9rparameters:", best_params)

from sklearn.model_selection import cross_val_score

# Example cross-validation for RandomForestClassifier
cv_scores = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)
from sklearn.model_selection import cross_val_score

# Example cross-validation for RandomForestClassifier
cv_scores = cross_val_score(RandomForestClassifier(n_estimators=100, random_state=42), X_train, y_train, cv=5)
print("Cross-Validation Scores:", cv_scores)

# Example feature importance for RandomForestClassifier
feature_importance = rf_classifier.feature_importances_
feature_names = X.columns

# Sort features by importance
feature_importance_sorted = sorted(zip(feature_names, feature_importance), key=lambda x: x[1], reverse=True)

# Display sorted feature importance
print("Feature Importance:")
for feature, importance in feature_importance_sorted:
    print(f"{feature}: {importance}")

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import StandardScaler

# Train RandomForestClassifier without handling imbalanced classes
rf_classifier_before = RandomForestClassifier(n_estimators=100, random_state=20)
rf_classifier_before.fit(X_train_scaled, y_train)
y_pred_before = rf_classifier_before.predict(X_test_scaled)

# Evaluate the model before handling
accuracy_before = accuracy_score(y_test, y_pred_before)
classification_rep_before = classification_report(y_test, y_pred_before)
conf_matrix_before = confusion_matrix(y_test, y_pred_before)

print("Before Handling Imbalanced Classes:")
print(f"Accuracy: {accuracy_before}")
print("Classification Report:\n", classification_rep_before)
print("Confusion Matrix:\n", conf_matrix_before)

# Apply RandomUnderSampler to handle imbalanced classes
sampler = RandomUnderSampler(random_state=20)
X_resampled, y_resampled = sampler.fit_resample(X_train_scaled, y_train)

# Train RandomForestClassifier after handling imbalanced classes
rf_classifier_after = RandomForestClassifier(n_estimators=100, random_state=20)
rf_classifier_after.fit(X_resampled, y_resampled)
y_pred_after = rf_classifier_after.predict(X_test_scaled)

# Evaluate the model after handling
accuracy_after = accuracy_score(y_test, y_pred_after)
classification_rep_after = classification_report(y_test, y_pred_after)
conf_matrix_after = confusion_matrix(y_test, y_pred_after)

print("\nAfter Handling Imbalanced Classes:")
print(f"Accuracy: {accuracy_after}")
print("Classification Report:\n", classification_rep_after)
print("Confusion Matrix:\n", conf_matrix_after)

import matplotlib.pyplot as plt
import seaborn as sns

# Create a bar plot to compare accuracy before and after handling imbalanced classes
accuracy_values = [accuracy_before, accuracy_after]
labels = ['Before Handling', 'After Handling']

plt.figure(figsize=(8, 6))
sns.barplot(x=labels, y=accuracy_values, palette='Set2')
plt.title('Accuracy Before and After Handling Imbalanced Classes')
plt.ylabel('Accuracy')
plt.show()

"""# **Logistic Regression Model**"""

# Standardize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train the Logistic Regression model
logistic_regression_model = LogisticRegression(random_state=20)
logistic_regression_model.fit(X_train_scaled, y_train)

# Make Predictions on the Test Set
y_pred = logistic_regression_model.predict(X_test_scaled)

# Evaluate the Model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)

from sklearn.metrics import classification_report

y_pred = logistic_regression_model.predict(X_test_scaled)
print(classification_report(y_test, y_pred))

print(f"Accuracy: {accuracy}")
print("Classification Report:\n", classification_rep)
print("Confusion Matrix:\n", conf_matrix)
# Evaluate the model after handling
accuracy_after = accuracy_score(y_test, y_pred_after)
classification_rep_after = classification_report(y_test, y_pred_after)
conf_matrix_after = confusion_matrix(y_test, y_pred_after)

print("\nAfter Handling Imbalanced Classes:")
print(f"Accuracy: {accuracy_after}")
print("Classification Report:\n", classification_rep_after)
print("Confusion Matrix:\n", conf_matrix_after)

# Before handling imbalanced classes with Logistic Regression
logistic_regression_model_before = LogisticRegression(random_state=20)
logistic_regression_model_before.fit(X_train_scaled, y_train)
y_pred_before_lr = logistic_regression_model_before.predict(X_test_scaled)

# Evaluate the Logistic Regression model before handling
accuracy_before_lr = accuracy_score(y_test, y_pred_before_lr)
classification_rep_before_lr = classification_report(y_test, y_pred_before_lr)
conf_matrix_before_lr = confusion_matrix(y_test, y_pred_before_lr)

print("Before Handling Imbalanced Classes with Logistic Regression:")
print(f"Accuracy: {accuracy_before_lr}")
print("Classification Report:\n", classification_rep_before_lr)
print("Confusion Matrix:\n", conf_matrix_before_lr)

# Apply RandomUnderSampler to handle imbalanced classes
sampler = RandomUnderSampler(random_state=20)
X_resampled_lr, y_resampled_lr = sampler.fit_resample(X_train_scaled, y_train)

# Train Logistic Regression after handling imbalanced classes
logistic_regression_model_after = LogisticRegression(random_state=20)
logistic_regression_model_after.fit(X_resampled_lr, y_resampled_lr)
y_pred_after_lr = logistic_regression_model_after.predict(X_test_scaled)

# Evaluate the Logistic Regression model after handling
accuracy_after_lr = accuracy_score(y_test, y_pred_after_lr)
classification_rep_after_lr = classification_report(y_test, y_pred_after_lr)
conf_matrix_after_lr = confusion_matrix(y_test, y_pred_after_lr)

print("\nAfter Handling Imbalanced Classes with Logistic Regression:")
print(f"Accuracy: {accuracy_after_lr}")
print("Classification Report:\n", classification_rep_after_lr)
print("Confusion Matrix:\n", conf_matrix_after_lr)

from sklearn.model_selection import GridSearchCV

# Standardize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define the Logistic Regression model
logistic_regression_model = LogisticRegression(random_state=20)

# Define the parameter grid for grid search
param_grid = {
    'penalty': ['l1', 'l2'],
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
}

# Create the grid search object
grid_search = GridSearchCV(logistic_regression_model, param_grid, cv=5, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X_train_scaled, y_train)

# Get the best parameters and model from the grid search
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Make predictions on the test set using the best model
y_pred_best = best_model.predict(X_test_scaled)

# Evaluate the best model
accuracy_best = accuracy_score(y_test, y_pred_best)
classification_rep_best = classification_report(y_test, y_pred_best)
conf_matrix_best = confusion_matrix(y_test, y_pred_best)

# Print the results
print("Best Parameters:", best_params)
print("Accuracy with Best Model:", accuracy_best)
print("Classification Report with Best Model:\n", classification_rep_best)
print("Confusion Matrix with Best Model:\n", conf_matrix_best)

import matplotlib.pyplot as plt
import seaborn as sns

# Data
accuracy_values = [
    accuracy_before, accuracy_after, accuracy_before_lr, accuracy_after_lr
]
labels = [
    'RandomForest Before', 'RandomForest After',
    'Logistic Regression Before', 'Logistic Regression After'
]

# Create a bar plot to compare accuracy before and after handling imbalanced classes for both models
plt.figure(figsize=(10, 6))
sns.barplot(x=labels, y=accuracy_values, palette='Set2')
plt.title('Accuracy Before and After Handling Imbalanced Classes')
plt.ylabel('Accuracy')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""# Initialize the SVM Model"""

# Train the Support Vector Machine (SVM) model
svm_model = SVC(kernel='linear', random_state=20)  # Use 'linear' kernel for simplicity, adjust based on your data
svm_model.fit(X_train_scaled, y_train)

# Make Predictions on the Test Set
y_pred_svm = svm_model.predict(X_test_scaled)

# Evaluate the Model
accuracy_svm = accuracy_score(y_test, y_pred_svm)
classification_rep_svm = classification_report(y_test, y_pred_svm)
conf_matrix_svm = confusion_matrix(y_test, y_pred_svm)

print(f"SVM Accuracy: {accuracy_svm}")
print("SVM Classification Report:\n", classification_rep_svm)
print("SVM Confusion Matrix:\n", conf_matrix_svm)

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.preprocessing import StandardScaler
from imblearn.under_sampling import RandomUnderSampler

def train_and_evaluate_svm(X_train, y_train, X_test, y_test):
    # Standardize Features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Define the parameter grid for grid search
    param_grid = {
        'C': [0.1, 1, 10],
        'gamma': [0.01, 0.1, 1],
    }

    # Create and fit the grid search object
    grid_search = GridSearchCV(SVC(kernel='linear', random_state=20), param_grid, cv=5, scoring='accuracy')
    grid_search.fit(X_train_scaled, y_train)

    # Get the best parameters and model from the grid search
    best_params_svm = grid_search.best_params_
    best_model_svm = grid_search.best_estimator_

    # Evaluate the best model on the test set
    y_pred_best_svm = best_model_svm.predict(X_test_scaled)

    # Calculate evaluation metrics
    accuracy_best_svm = accuracy_score(y_test, y_pred_best_svm)
    classification_rep_best_svm = classification_report(y_test, y_pred_best_svm)
    conf_matrix_best_svm = confusion_matrix(y_test, y_pred_best_svm)

    # Print the results
    print("Best Parameters for SVM:", best_params_svm)
    print("Accuracy with Best SVM Model:", accuracy_best_svm)
    print("Classification Report with Best SVM Model:\n", classification_rep_best_svm)
    print("Confusion Matrix with Best SVM Model:\n", conf_matrix_best_svm)

# Example usage:
# train_and_evaluate_svm(X_train, y_train, X_test, y_test)

from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import StandardScaler

# Before handling imbalanced classes with SVM
svm_model_before = SVC(kernel='linear', random_state=20)
svm_model_before.fit(X_train_scaled, y_train)
y_pred_before_svm = svm_model_before.predict(X_test_scaled)

# Evaluate the SVM model before handling
accuracy_before_svm = accuracy_score(y_test, y_pred_before_svm)
classification_rep_before_svm = classification_report(y_test, y_pred_before_svm)
conf_matrix_before_svm = confusion_matrix(y_test, y_pred_before_svm)

print("Before Handling Imbalanced Classes with SVM:")
print(f"Accuracy: {accuracy_before_svm}")
print("Classification Report:\n", classification_rep_before_svm)
print("Confusion Matrix:\n", conf_matrix_before_svm)

# Apply RandomUnderSampler to handle imbalanced classes
sampler = RandomUnderSampler(random_state=20)
X_resampled_svm, y_resampled_svm = sampler.fit_resample(X_train_scaled, y_train)

# Train SVM after handling imbalanced classes
svm_model_after = SVC(kernel='linear', random_state=20)
svm_model_after.fit(X_resampled_svm, y_resampled_svm)
y_pred_after_svm = svm_model_after.predict(X_test_scaled)

# Evaluate the SVM model after handling
accuracy_after_svm = accuracy_score(y_test, y_pred_after_svm)
classification_rep_after_svm = classification_report(y_test, y_pred_after_svm)
conf_matrix_after_svm = confusion_matrix(y_test, y_pred_after_svm)

print("\nAfter Handling Imbalanced Classes with SVM:")
print(f"Accuracy: {accuracy_after_svm}")
print("Classification Report:\n", classification_rep_after_svm)
print("Confusion Matrix:\n", conf_matrix_after_svm)

import matplotlib.pyplot as plt
import seaborn as sns

# Assuming accuracy_before_svm and accuracy_after_svm are defined
accuracy_before_svm = 0.85  # Replace with your actual value
accuracy_after_svm = 0.92   # Replace with your actual value

# Data for SVM
accuracy_values_svm = [accuracy_before_svm, accuracy_after_svm]
labels_svm = ['SVM Before', 'SVM After']

# Create a bar plot to compare accuracy before and after handling imbalanced classes for the SVM model
plt.figure(figsize=(8, 5))
sns.barplot(x=labels_svm, y=accuracy_values_svm, palette='Set3')
plt.title('Accuracy Before and After Handling Imbalanced Classes (SVM)')
plt.ylabel('Accuracy')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Standardize Features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Build the Neural Network **Model**"""

# Build the Neural Network
model = Sequential()

# Input layer
model.add(Dense(units=128, activation='relu', input_dim=X_train_scaled.shape[1]))

# Hidden layers
model.add(Dense(units=64, activation='relu'))
model.add(Dense(units=32, activation='relu'))

# Output layer
model.add(Dense(units=1, activation='sigmoid'))

"""
# The Adam optimizer is a popular optimization algorithm used in training neural networks"""

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the Neural Network
model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))

# Evaluate the Model
y_pred_prob = model.predict(X_test_scaled)
y_pred_nn = (y_pred_prob > 0.5).astype(int).flatten()

accuracy_nn = accuracy_score(y_test, y_pred_nn)
classification_rep_nn = classification_report(y_test, y_pred_nn)
conf_matrix_nn = confusion_matrix(y_test, y_pred_nn)

print(f"Neural Network Accuracy: {accuracy_nn}")
print("Neural Network Classification Report:\n", classification_rep_nn)
print("Neural Network Confusion Matrix:\n", conf_matrix_nn)

from keras.models import Sequential
from keras.layers import Dense
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from imblearn.under_sampling import RandomUnderSampler
from sklearn.preprocessing import StandardScaler

# Before handling imbalanced classes with Neural Network
model_before = Sequential()
model_before.add(Dense(units=128, activation='relu', input_dim=X_train_scaled.shape[1]))
model_before.add(Dense(units=64, activation='relu'))
model_before.add(Dense(units=32, activation='relu'))
model_before.add(Dense(units=1, activation='sigmoid'))

model_before.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history_before = model_before.fit(X_train_scaled, y_train, epochs=20, batch_size=64, validation_data=(X_test_scaled, y_test))

# Evaluate the Neural Network before handling
y_pred_prob_before = model_before.predict(X_test_scaled)
y_pred_nn_before = (y_pred_prob_before > 0.5).astype(int).flatten()

accuracy_nn_before = accuracy_score(y_test, y_pred_nn_before)
classification_rep_nn_before = classification_report(y_test, y_pred_nn_before)
conf_matrix_nn_before = confusion_matrix(y_test, y_pred_nn_before)

print("Before Handling Imbalanced Classes with Neural Network:")
print(f"Accuracy: {accuracy_nn_before}")
print("Classification Report:\n", classification_rep_nn_before)
print("Confusion Matrix:\n", conf_matrix_nn_before)

# Apply RandomUnderSampler to handle imbalanced classes
sampler = RandomUnderSampler(random_state=20)
X_resampled_nn, y_resampled_nn = sampler.fit_resample(X_train_scaled, y_train)

# Train Neural Network after handling imbalanced classes
model_after = Sequential()
model_after.add(Dense(units=128, activation='relu', input_dim=X_train_scaled.shape[1]))
model_after.add(Dense(units=64, activation='relu'))
model_after.add(Dense(units=32, activation='relu'))
model_after.add(Dense(units=1, activation='sigmoid'))

model_after.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

history_after = model_after.fit(X_resampled_nn, y_resampled_nn, epochs=20, batch_size=64, validation_data=(X_test_scaled, y_test))

# Evaluate the Neural Network after handling
y_pred_prob_after = model_after.predict(X_test_scaled)
y_pred_nn_after = (y_pred_prob_after > 0.5).astype(int).flatten()

accuracy_nn_after = accuracy_score(y_test, y_pred_nn_after)
classification_rep_nn_after = classification_report(y_test, y_pred_nn_after)
conf_matrix_nn_after = confusion_matrix(y_test, y_pred_nn_after)

print("\nAfter Handling Imbalanced Classes with Neural Network:")
print(f"Accuracy: {accuracy_nn_after}")
print("Classification Report:\n", classification_rep_nn_after)
print("Confusion Matrix:\n", conf_matrix_nn_after)

from keras.callbacks import EarlyStopping


# Before handling imbalanced classes with Neural Network
model_before = Sequential()
model_before.add(Dense(units=128, activation='relu', input_dim=X_train_scaled.shape[1]))
model_before.add(Dense(units=64, activation='relu'))
model_before.add(Dense(units=32, activation='relu'))
model_before.add(Dense(units=1, activation='sigmoid'))

model_before.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Add EarlyStopping callback
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history_before = model_before.fit(X_train_scaled, y_train, epochs=200, batch_size=64,
                                  validation_data=(X_test_scaled, y_test),
                                  callbacks=[early_stopping])

# Evaluate the Neural Network before handling
y_pred_prob_before = model_before.predict(X_test_scaled)
y_pred_nn_before = (y_pred_prob_before > 0.5).astype(int).flatten()

accuracy_nn_before = accuracy_score(y_test, y_pred_nn_before)
classification_rep_nn_before = classification_report(y_test, y_pred_nn_before)
conf_matrix_nn_before = confusion_matrix(y_test, y_pred_nn_before)

print("Before Handling Imbalanced Classes with Neural Network:")
print(f"Accuracy: {accuracy_nn_before}")
print("Classification Report:\n", classification_rep_nn_before)
print("Confusion Matrix:\n", conf_matrix_nn_before)

# Apply RandomUnderSampler to handle imbalanced classes
sampler = RandomUnderSampler(random_state=20)
X_resampled_nn, y_resampled_nn = sampler.fit_resample(X_train_scaled, y_train)

# Train Neural Network after handling imbalanced classes
model_after = Sequential()
model_after.add(Dense(units=128, activation='relu', input_dim=X_train_scaled.shape[1]))
model_after.add(Dense(units=64, activation='relu'))
model_after.add(Dense(units=32, activation='relu'))
model_after.add(Dense(units=1, activation='sigmoid'))

model_after.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Add EarlyStopping callback
history_after = model_after.fit(X_resampled_nn, y_resampled_nn, epochs=20, batch_size=64,
                                validation_data=(X_test_scaled, y_test),
                                callbacks=[early_stopping])

# Evaluate the Neural Network after handling
y_pred_prob_after = model_after.predict(X_test_scaled)
y_pred_nn_after = (y_pred_prob_after > 0.5).astype(int).flatten()

accuracy_nn_after = accuracy_score(y_test, y_pred_nn_after)
classification_rep_nn_after = classification_report(y_test, y_pred_nn_after)
conf_matrix_nn_after = confusion_matrix(y_test, y_pred_nn_after)

print("\nAfter Handling Imbalanced Classes with Neural Network:")
print(f"Accuracy: {accuracy_nn_after}")
print("Classification Report:\n", classification_rep_nn_after)
print("Confusion Matrix:\n", conf_matrix_nn_after)

import matplotlib.pyplot as plt

# Extract loss values from the training history
loss_before = history_before.history['loss']
val_loss_before = history_before.history['val_loss']

loss_after = history_after.history['loss']
val_loss_after = history_after.history['val_loss']

# Plot the training and validation loss over epochs
plt.figure(figsize=(10, 6))
plt.plot(loss_before, label='Training Loss (Before)')
plt.plot(val_loss_before, label='Validation Loss (Before)')
plt.plot(loss_after, label='Training Loss (After)')
plt.plot(val_loss_after, label='Validation Loss (After)')

plt.title('Training and Validation Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Function to plot confusion matrix
def plot_confusion_matrix_custom(y_true, y_pred, classes, cmap='Blues'):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Usage example for Random Forest
plot_confusion_matrix_custom(y_test, y_pred, classes=['Not Subscribed', 'Subscribed'])

import matplotlib.pyplot as plt

# List of models and their corresponding accuracy scores
models = ['Random Forest', 'SVM', 'Neural Network', 'Logistic Regression']
accuracy_scores = [accuracy, accuracy_svm, accuracy_nn, accuracy_after_lr]

# Create a bar plot for accuracy comparison
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy_scores, color=['blue', 'green', 'orange', 'purple'])
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison Before Handling Imbalanced Classes')
plt.show()

# Visualize classification report comparison
classification_reports = [classification_rep, classification_rep_svm, classification_rep_nn, classification_rep_after_lr]

for model, report in zip(models, classification_reports):
    print(f"Model: {model}\n")
    print(report)
    print("----------------------------------------------")

# Confusion matrix comparison
confusion_matrices = [conf_matrix, conf_matrix_svm, conf_matrix_nn, conf_matrix_after_lr]

for model, matrix in zip(models, confusion_matrices):
    print(f"Model: {model}\n")
    print(matrix)
    print("----------------------------------------------")

import matplotlib.pyplot as plt

# List of models and their corresponding accuracy scores
models = ['Random Forest', 'SVM', 'Logistic Regression', 'Neural Network']
accuracy_scores = [accuracy_after, accuracy_after_svm, accuracy_after_lr, accuracy_nn_after]

# Create a bar plot
plt.figure(figsize=(10, 6))
plt.bar(models, accuracy_scores, color=['blue', 'green', 'purple', 'orange'])
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.title('Model Accuracy Comparison After Handling Imbalanced Classes')
plt.ylim(0, 1)  # Set the y-axis range from 0 to 1 for accuracy values
plt.show()

import matplotlib.pyplot as plt

# List of models and their corresponding accuracy scores
models = ['Random Forest', 'SVM', 'Logistic Regression', 'Neural Network']
accuracy_scores = [accuracy_after, accuracy_after_svm, accuracy_after_lr, accuracy_nn_after]

# Convert accuracy scores to percentages
accuracy_percentages = [score * 100 for score in accuracy_scores]

# Create a bar plot
plt.figure(figsize=(10, 6))
bars = plt.bar(models, accuracy_percentages, color=['blue', 'green', 'purple', 'orange'])
plt.xlabel('Models')
plt.ylabel('Accuracy (%)')
plt.title('Model Accuracy Comparison After Handling Imbalanced Classes')
plt.ylim(0, 100)  # Set the y-axis range from 0 to 100 for percentage values

# Add percentage labels on top of each bar
for bar, percentage in zip(bars, accuracy_percentages):
    plt.text(bar.get_x() + bar.get_width() / 2 - 0.15, bar.get_height() + 1,
             f'{percentage:.2f}%', ha='center', va='bottom', color='black')

plt.show()